# MIAM: Multimodal Industrial Activity Monitoring 
## Overview
The **MIAM Dataset** is a comprehensive multimodal dataset designed to facilitate research in human-robot collaboration, activity recognition, and engagement prediction within industrial environments. Captured during realistic assembly and disassembly workflows, the dataset offers a rich resource for advancing industrial automation, human behavior modeling, and collaborative robotics.

## Features
- **Multimodal Data**: Includes RGB, depth, and 9-axis IMU sensor data from both hands.
- **Annotations**: Provides action labels, engagement states, and timestamps for over 22 sessions.
- **Realistic Scenarios**: Simulates real-world tasks with natural operator variability.
- **Gender-Diverse Subjects**: Includes data from 8 participants across different demographics.
- **Untrimmed Videos**: Covers 220 minutes of industrial activity.

## Applications
The dataset supports a variety of research tasks:
- Action recognition and localization
- Engagement prediction
- Human-robot interaction and collaboration modeling

## Dataset Structure
The dataset is organized into the following components:
- `RGB/`: Multi-view RGB video files
- `Depth/`: Synchronized depth frames
- `IMU/`: Motion sensor data
- `Annotations/`: JSON files with labeled actions, engagement states, and timestamps

## Recording Setup
- **Cameras**: Logitech Brio 4K and Intel RealSense D455
- **IMU Sensors**: Worn on both hands for motion capture
- **Annotation Tool**: VGG Image Annotator (VIA)

## Usage
1. Clone the repository:
   ```bash
   git clone https://github.com/navalkishoremehta95/MIAM.git


## contact
naval.ceeri18a@acsir.res.in, sanjay@ceeri.res.in

   

 
